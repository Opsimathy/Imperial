{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCglJPPsq335"
      },
      "source": [
        "# Graph-based learning\n",
        "\n",
        "(This notebook is partly based on a workshop by Robert Peach and Mauricio Barahona: https://github.com/peach-lucien/networks_workshop/tree/main)\n",
        "\n",
        "Here, we will go through an analysis of the *Caenorhabditis elegans* (C. elgans) connectome. C. elegans is the only organism for which the wiring diagram of its complete nervous system has been mapped with reasonable accuracy at the cellular level. Despite this structural information, which has been available for decades, it still proves difficult to understand the system, e.g, resolving the functional involvement of specific neurons in defined behavioural responses.\n",
        "\n",
        "We will use data that Robert Peach has reconstructed from the following article with the inclusion of muscles https://www.nature.com/articles/nature24056. The connectome is composed of directed connections from one neuron to another neuron in accordance with their biological influence.\n",
        "Each node falls into one of four categories:\n",
        "1. Sensory neurons (S)\n",
        "2. Inter neurons (I)\n",
        "3. Motor neurons (M)\n",
        "4. Muscles (U)\n",
        "\n",
        "Therefore, we can model the nematode nervous system as a *directed network* whose nodes include neurons and muscles, and whose links represent the electrical and chemical synaptic connections between them, including neuromuscular junctions. The weights of the edges correspond to the number of synaptic connections between a pair of neurons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwuAoYm1q_0K"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import numpy.testing as npt\n",
        "import pandas as pd\n",
        "\n",
        "# We set a global configuration for matplotlib to obtain well-readable plots\n",
        "SMALL_SIZE = 12\n",
        "MEDIUM_SIZE = 16\n",
        "BIGGER_SIZE = 20\n",
        "\n",
        "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
        "plt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the axes title\n",
        "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
        "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
        "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
        "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
        "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSbU0vtcFRoV"
      },
      "source": [
        "## Pre-processing C. Elegans network data\n",
        "\n",
        "We are provided with an adjacency matrix $A_\\text{raw}$ of the C. Elgans network and metadata for the nodes (including neuron type, name and neuron class)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcN3gGLpq2Nm"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = '/content/drive/My Drive/Methods for Data Science/2025/Lectures/9 Graphs/notebooks/'\n",
        "\n",
        "# import adjacency matrix\n",
        "A_raw = pd.read_csv(path+\"celegans_adjacency_transposed.csv\", index_col=0)\n",
        "A_raw = A_raw.to_numpy()\n",
        "\n",
        "# import node data\n",
        "node_data = pd.read_csv(path+\"celegans_nodedata.csv\", index_col=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSr0nDW8mXV2"
      },
      "source": [
        "We can first plot $A_\\text{raw}$ as a heatmap and observe that the matrix is very sparse with values ranging from 0 to over 30."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huDMULW5rGjh"
      },
      "outputs": [],
      "source": [
        "# plot raw adjacency matrix\n",
        "fig, ax = plt.subplots(1)\n",
        "im = ax.imshow(A_raw, cmap=\"binary\")\n",
        "plt.colorbar(im)\n",
        "ax.set(title=\"Raw adjacency matrix\", xlabel=r\"Node $i$\", ylabel=r\"Node $j$\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjDWYYR-myvr"
      },
      "source": [
        "Arguably, this visualisation of the adjacency matrix is not very illuminating. We will discuss better ways to visualise our data below. Let's first review the node metadata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RA0zmAXYZZee"
      },
      "outputs": [],
      "source": [
        "# look up node data\n",
        "node_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWDGDRaRm_qX"
      },
      "source": [
        "We encode the node types with numerical values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "273tWyR7rspx"
      },
      "outputs": [],
      "source": [
        "# transform node type into integers\n",
        "type_to_value = {\"S\" : 0, \"I\" : 1, \"M\" : 2, \"U\" : 3}\n",
        "value_to_type = {0 : \"S\", 1: \"I\", 2 : \"M\", 3 : \"U\"}\n",
        "node_type = node_data[\"type\"].map(type_to_value)\n",
        "node_type = np.asarray(node_type)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SUhSdI-nKL5"
      },
      "source": [
        "Let us check some basic properties of the graph, in particular whether is undirected and unweighted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xlN-DVSrMJG"
      },
      "outputs": [],
      "source": [
        "# check properties of graph\n",
        "print(\"Graph is undirected:\", np.array_equal(A_raw, A_raw.T))\n",
        "print(\"Graph is unweighted:\", np.array_equal(np.unique(A_raw), np.arange(2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiaoUCY_bCUm"
      },
      "source": [
        "To simply our analysis below, we go from the raw adjacency matrix $A_\\text{raw}$ to the adjacency matrix $A$ of an undirected and unweighted graph, where $A_{ij}=A_{ji}=1$ if either $A_{\\text{raw}, ij}>0$ or $A_{\\text{raw}, ji}>0$ and $A_{ij}=A_{ji}=0$ otherwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3sERpMv9B1p"
      },
      "outputs": [],
      "source": [
        "## EDIT THIS CELL\n",
        "\n",
        "# we go to undirected unweighted graph\n",
        "A = # <-- EDIT THIS LINE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nh_FdvAz8Syi"
      },
      "outputs": [],
      "source": [
        "# double check if graph is now undirected and unweighted\n",
        "print(\"Graph is undirected:\", np.array_equal(A, A.T))\n",
        "print(\"Graph is unweighted:\", np.array_equal(np.unique(A), np.arange(2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-fwU9f5FWP8"
      },
      "source": [
        "## Degree distribution of undirected C. Elegans network\n",
        "\n",
        "Let us now compute the number of nodes and eges of our undirected unweighted graph given by $A$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-26G5RVlYDPA"
      },
      "outputs": [],
      "source": [
        "## EDIT THIS CELL\n",
        "\n",
        "# compute number of nodes\n",
        "N = # <-- EDIT THIS LINE\n",
        "print(\"Number of nodes:\", N)\n",
        "\n",
        "# compute number of edges\n",
        "M = # <-- EDIT THIS LINE\n",
        "print(\"Number of edges:\", M)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37CrWGE1qhZl"
      },
      "source": [
        "As a first step of our analysis we look at the degree distribution $\\boldsymbol{d}=A\\boldsymbol{1}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnfEHW-CFuWP"
      },
      "outputs": [],
      "source": [
        "## EDIT THIS CELL\n",
        "\n",
        "# compute degrees\n",
        "d = # <-- EDIT THIS LINE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Un36PmUDrw9_"
      },
      "outputs": [],
      "source": [
        "# visualise degree distributions\n",
        "fig, ax = plt.subplots(1)\n",
        "bins = np.arange(0, d.max()+2,2)\n",
        "ax.hist(d, bins=bins, alpha=0.5, edgecolor='black')\n",
        "ax.set(xlabel=\"Degree\", ylabel=\"Frequency\", title=\"Degree distribution\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAO9y-Dkysyh"
      },
      "source": [
        "Let us look up the names and types two highest degree nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ad3R8r9nyS6u"
      },
      "outputs": [],
      "source": [
        "## EDIT THIS CELL\n",
        "\n",
        "# check which node has highest degree\n",
        "node1 = # <-- EDIT THIS LINE\n",
        "print(f\"Highest degree neuron is {node1['name']} of type {node1['type']}.\") # <-- SOLUTION\n",
        "\n",
        "# check which node has second highest degree\n",
        "node2 = # <-- EDIT THIS LINE\n",
        "print(f\"Second highest degree neuron is {node2['name']} of type {node2['type']}.\")  # <-- SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f-IalAIYT-V"
      },
      "source": [
        "We notice that the two highest degree nodes are both of the same type inter neuron type (I). This suggests that it will be instructive to plot the degree distribution for the four different types of nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qePyHMuEy_jn"
      },
      "outputs": [],
      "source": [
        "# visualise degree distributions for the four different types\n",
        "fig, ax = plt.subplots(1)\n",
        "bins = np.arange(0,d.max()+2,2)\n",
        "ax.hist(d[node_type==0], bins=bins,alpha=0.5, edgecolor='black', label=r\"S\")\n",
        "ax.hist(d[node_type==1], bins=bins,alpha=0.5, edgecolor='black', label=r\"I\")\n",
        "ax.hist(d[node_type==2], bins=bins,alpha=0.5, edgecolor='black', label=r\"M\")\n",
        "ax.hist(d[node_type==3], bins=bins,alpha=0.5, edgecolor='black', label=r\"U\")\n",
        "ax.set(xlabel=\"Degree\",ylabel=\"Frequency\",title=\"Degree distribution for different types\")\n",
        "ax.legend(title=\"Types\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNxPisNnfX_1"
      },
      "source": [
        "From the degree distrubtions we can observe that the inter neurons (I) seem to have a higher degree, i.e., they are most connected, and the muscles (U) are the least connected in the nervous system.\n",
        "\n",
        "We will define node colours consistent with this plot to use for later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-gIVr8kfdb4"
      },
      "outputs": [],
      "source": [
        "# we define node colours consistent with the plot above\n",
        "cmap = matplotlib.colormaps.get_cmap('tab10')\n",
        "color_type = [cmap(i) for i in node_type]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpKqC6se2yKb"
      },
      "source": [
        "## Spectral analysis of symmetric normalised Laplacian\n",
        "\n",
        "We focus our analysis on the spectrum of the normalised graph Laplacian $L_\\text{sym}$, which is given by:\n",
        "\n",
        "$$L_\\text{sym}=D^{-1/2} L D^{-1/2},$$\n",
        "\n",
        "where $L=D-A$ is the combinatorial Laplacian and $D$ the diagonal degree matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x36_408e2VNV"
      },
      "outputs": [],
      "source": [
        "## EDIT THIS CELL\n",
        "\n",
        "# define diagonal degree matrix\n",
        "D = # <-- EDIT THIS LINE\n",
        "\n",
        "# compute combinatorial Laplacian\n",
        "L = # <-- EDIT THIS LINE\n",
        "\n",
        "# compute square root of inverse diagonal degree matrix\n",
        "D_sqrt_inv = # <-- EDIT THIS LINE\n",
        "\n",
        "# compute symmetric normalised Laplacian\n",
        "L_s = # <-- EDIT THIS LINE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNYKNqyXYmkz"
      },
      "source": [
        "We are now interested in the spectral decomposition of $L_\\text{sym}$. We know that the first (smallest) eigenvalue is $\\lambda_1=0$ with corresponding eigenvector $\\boldsymbol{v}_1=D^{1/2}\\boldsymbol{1}$ (can you prove this?)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOCgaGAkwD5O"
      },
      "outputs": [],
      "source": [
        "# the square-root of the degree vector is the zero eigenvector to eigenvalue 0\n",
        "np.allclose(L_s @ np.sqrt(d),np.zeros(N))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qb-1KYjaaNsT"
      },
      "source": [
        "To compute the full spectrum of $L_\\text{sym}$, we use the fact that it is a real symmetric matrix and sort the eigenvalues (and corresponding eigenvectors) in ascending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVb_uB3Z3YR9"
      },
      "outputs": [],
      "source": [
        "## EDIT THIS CELL\n",
        "\n",
        "# compute eigen decomposition using the fact that L_s is symmetric\n",
        "eigenvals, eigenvecs = # <-- EDIT THIS LINE\n",
        "\n",
        "# sort eigenvalues and corresponding eigenvector\n",
        "eigenvecs = # <-- EDIT THIS LINE\n",
        "eigenvals = # <-- EDIT THIS LINE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59WseLUhdyTv"
      },
      "source": [
        "We can visualise the spectrum of $L_\\text{sym}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5L5-lRG4wFAL"
      },
      "outputs": [],
      "source": [
        "# plot eigenvalues\n",
        "fig, ax = plt.subplots(1)\n",
        "ax.plot(np.arange(1, N+1), eigenvals)\n",
        "ax.set(title=\"Eigenvalues of symmetric normalised Laplacian\", xlabel=r\"$i$\", ylabel=r\"$\\lambda_i$\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9gJSHqVa7XH"
      },
      "source": [
        "We confirm that the first eigenvalue is $\\lambda_1=0$, but we are more interested in the second eigenvalue $\\lambda_2>0$, which is also called the algebraic connectivity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fvdMIC35Y2Q"
      },
      "outputs": [],
      "source": [
        "## EDIT THIS CELL\n",
        "\n",
        "# print first and second eigenvalues\n",
        "print(\"First eigenvalue:\", ... )    # <-- EDIT THIS LINE\n",
        "print(\"Second eigenvalue:\", ... )   # <-- EDIT THIS LINE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvpotnPXihC2"
      },
      "source": [
        "As $\\lambda_2$, is small we expect that the graph has a good bipartition. We will study this later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rIc7nB-tI_C"
      },
      "source": [
        "### Graph visualisation using Laplacian eigenmaps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8iCTOSDbiCI"
      },
      "source": [
        "But first we will use the second (Fiedler) eigenvector $\\boldsymbol{v}_2$ and third eigengenvector $\\boldsymbol{v}_3$ to visualise the network. Using these two Laplacian eigenvectors (also called Laplacian eigenmaps) as $\\boldsymbol{x}$ and $\\boldsymbol{y}$ coordinates of the nodes gives us a good two-dimensional representation of the graph (a spectral embedding). We can improve the visualisation when normalising the coordinates such that $\\boldsymbol{x}=D^{-1/2}\\boldsymbol{v}_2$ and $\\boldsymbol{y}=D^{-1/2}\\boldsymbol{v}_3$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5VaJ71Ga6c9"
      },
      "outputs": [],
      "source": [
        "## EDIT THIS CELL\n",
        "\n",
        "# get second (Fiedler) and third eigenvectors\n",
        "v2 = # <-- EDIT THIS LINE\n",
        "v3 = # <-- EDIT THIS LINE\n",
        "\n",
        "# normalise coordinates\n",
        "x = # <-- EDIT THIS LINE\n",
        "y = # <-- EDIT THIS LINE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI4ZK5HDeyTh"
      },
      "source": [
        "We can simply plot the graph by drawing lines between the embeddings of two connected nodes. Additionally, we can scale the node size dependent on the degree. Below we provide a function that can be used for plotting the graph."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graph(adjacency, x_coord, y_coord, node_size, node_color):\n",
        "    \"\"\"Plot undirected graph.\n",
        "\n",
        "    Parameters:\n",
        "        adjacency (np.array): N x N adjacency matrix.\n",
        "        x_coord (np.array): X-coordinates for N nodes.\n",
        "        y_coord (np.array): Y-coordinates for N nodes.\n",
        "        node_size (np.array): Size for each of the N nodes.\n",
        "        node_color (np.array): Color for each of the N nodes.\n",
        "    \"\"\"\n",
        "\n",
        "    # plot\n",
        "    fig, ax = plt.subplots(1)\n",
        "\n",
        "    # plot edges\n",
        "    for i in range(N):\n",
        "        for j in range(i+1,N):\n",
        "            if adjacency[i,j] > 0:\n",
        "                ax.plot([x_coord[i],x_coord[j]],[y_coord[i],y_coord[j]], color=\"black\", alpha=0.5, linewidth=0.3)\n",
        "\n",
        "    # plot nodes\n",
        "    scatter = ax.scatter(x_coord,y_coord,s=0.2*node_size, c=node_color, zorder=10)\n",
        "\n",
        "    return ax"
      ],
      "metadata": {
        "id": "HdL_8hmOgAas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now plot the graph with nodes coloured according to type."
      ],
      "metadata": {
        "id": "QwSavANxirj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot graph\n",
        "ax = plot_graph(adjacency=A, x_coord=x, y_coord=y, node_size=d, node_color=color_type)\n",
        "\n",
        "# set labels\n",
        "ax.set(xlabel=\"Normalised Fiedler eigenvector\", ylabel=\"Normalised third eigenvector\", title=\"Spectral graph layout\")\n",
        "\n",
        "# create legend for node types\n",
        "types_legend = [plt.Line2D([0], [0], marker='o', color='w', label=label, markerfacecolor=cmap(value)) for value, label in value_to_type.items()]\n",
        "plt.legend(handles=types_legend, title='Types');\n"
      ],
      "metadata": {
        "id": "LZbnSXGngeke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6oQoPelgB4-"
      },
      "source": [
        "We observe that the muscles (U) are located in the periphery of the network, while the most connected inter neurons (I) are located in the center.\n",
        "\n",
        "Another way to visualise the graph is to directly use the node degree $\\boldsymbol{d}$ as the y-coordinate, i.e., $\\boldsymbol{y}=\\boldsymbol{d}$."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot graph\n",
        "ax = plot_graph(adjacency=A, x_coord=x, y_coord=d, node_size=d, node_color=color_type)\n",
        "\n",
        "# set labels\n",
        "ax.set(xlabel=\"Normalised Fiedler eigenvector\", ylabel=\"Degree\", title=\"Spectral graph layout with degree\")\n",
        "\n",
        "# create legend for node types\n",
        "types_legend = [plt.Line2D([0], [0], marker='o', color='w', label=label, markerfacecolor=cmap(value)) for value, label in value_to_type.items()]\n",
        "plt.legend(handles=types_legend, title='Types');\n"
      ],
      "metadata": {
        "id": "T0TNYh9Nhi3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxz44A1shPsX"
      },
      "source": [
        "This visualisation gives a more decluttered picture and is very useful to distinguish the different node types in the network. For the rest of the notebook, we will thus use this visualisation.\n",
        "\n",
        "As you have learnt by now, graph visualisation is a very interesting topic in itself and there are many algorithms to plot various graphs, including very large ones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "advh44xpwmRC"
      },
      "source": [
        "### Questions:\n",
        "\n",
        "- Why is graph visualisation not unique?\n",
        "- Can you come up with alternative graph visualisations?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEv-c64QtCML"
      },
      "source": [
        "### Bipartitioning using Fiedler eigenvector\n",
        "\n",
        "We will now use the second (Fiedler) eigenvector $\\boldsymbol{v}_2$ to bipartition the network into two communities, given by the sign of $\\boldsymbol{v}_2$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwbjjxE74T2D"
      },
      "outputs": [],
      "source": [
        "## EDIT THIS CELL\n",
        "\n",
        "# bipartition is given by sign of Fiedler eigenvector\n",
        "bipartition = np.asarray(v2>0, dtype=int)       # <-- SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-i9nvf9zh_Kr"
      },
      "source": [
        "As the algebraic connectivity $\\lambda_2$ is small, we expect that the bipartition is very balanced. To confirm this we compute the number of nodes and the total degree of both communities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dx2UgEsX3oVM"
      },
      "outputs": [],
      "source": [
        "## EDIT THIS CELL\n",
        "\n",
        "# compute statistics for bipartition\n",
        "print(\"Size of first community:\", ...)               # <-- EDIT THIS LINE\n",
        "print(\"Total degree of first community:\", ...)       # <-- EDIT THIS LINE\n",
        "\n",
        "print(\"\\nSize of second community:\", ...)            # <-- EDIT THIS LINE\n",
        "print(\"Total degree of second community:\", ...)      # <-- EDIT THIS LINE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIhszMB4moqO"
      },
      "source": [
        "Indeed, we find that the communities of the bipartition contain a very similar number of nodes and also have a similar total degree. It is illustrative to visualise the bipartition in the network."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use different colours for bipartition\n",
        "cmap_binary = {0 : \"purple\", 1 : \"yellow\"}\n",
        "color_bipartition = [cmap_binary[i] for i in bipartition]\n",
        "\n",
        "# plot graph\n",
        "ax = plot_graph(adjacency=A, x_coord=x, y_coord=d, node_size=d, node_color=color_bipartition)\n",
        "\n",
        "# plot cut induced by Fiedler eigenvector\n",
        "ax.axvline(x=0, color=\"red\")\n",
        "\n",
        "# create legend for bipartition\n",
        "bipartition_legend = [plt.Line2D([0], [0], marker='o', color='w', label=\"Community 1\", markerfacecolor=\"purple\"),\n",
        "                plt.Line2D([0], [0], marker='o', color='w', label=\"Community 2\", markerfacecolor=\"yellow\")]\n",
        "plt.legend(handles=bipartition_legend, title='Bipartition')\n",
        "\n",
        "# plot labels\n",
        "ax.set(xlabel=\"Normalised Fiedler eigenvector\", ylabel=\"Degree\", title=\"Normalised Fiedler bipartition\");\n"
      ],
      "metadata": {
        "id": "MRyLKwhDjhFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OOwHnilm-MH"
      },
      "source": [
        "While the nodes with the highest degree belong to community 2, the degree distribution seems to be quite uniform between the two communities. We confirm this by plotting the degree distributions as histograms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-ocvXK-fg_x"
      },
      "outputs": [],
      "source": [
        "# visualise degree distributions for both communities\n",
        "fig, ax = plt.subplots(1)\n",
        "bins = np.arange(0,d.max()+2,2)\n",
        "ax.hist(d[bipartition==0], bins=bins, alpha=0.5, edgecolor='black', label=r\"Community 1\", color=\"purple\")\n",
        "ax.hist(d[bipartition==1], bins=bins, alpha=0.5, edgecolor='black', label=r\"Community 2\", color=\"yellow\")\n",
        "ax.set(xlabel=\"Degree\", ylabel=\"Frequency\", title=\"Degree distribution for bipartition\")\n",
        "ax.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9MW77rj5kW6"
      },
      "source": [
        "## Spectral clustering using more Laplacian eigenvectors\n",
        "\n",
        "To go beyond bipartitions, we can use more Laplacian eigenvectors and apply what is called *spectral clustering*, where we apply $k$-means clustering to the Laplacian eigenmaps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Niuz-rqoq8rZ"
      },
      "source": [
        "### Code for $K$-means clustering from Week 7\n",
        "\n",
        "To use $K$-means clustering we first copy-paste the code from Week 7."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EDIT THIS FUNCTION\n",
        "def compute_within_distance(centroids, X, labels):\n",
        "    \"\"\"\n",
        "    Compute the within-cluster distance.\n",
        "\n",
        "    Parameters:\n",
        "        centroids (np.ndarray): Centroids matrix, shape (k, p).\n",
        "        X (np.ndarray): Samples matrix, shape (N, p).\n",
        "        labels (np.ndarray): Cluster index of each sample, shape (N,).\n",
        "\n",
        "    Retruns:\n",
        "        within_distance (float): Within-cluster distance.\n",
        "    \"\"\"\n",
        "\n",
        "    within_distance = 0.0\n",
        "    K, p = centroids.shape\n",
        "\n",
        "    # calculate the contribution to W(C) for each cluster\n",
        "    for l in range(len(centroids)):\n",
        "        centroid = centroids[l]\n",
        "        # skip empty clusters\n",
        "        if np.isinf(centroid).any():\n",
        "            continue\n",
        "\n",
        "        # Select samples belonging to label=l.\n",
        "        X_cluster = X[labels == l]\n",
        "\n",
        "        # We need to add the `X_cluster` contribution to `within_distance`\n",
        "        # Compute the W(C) contribution for each sample\n",
        "        cluster_se = (X_cluster - centroid)**2 # <-- SOLUTION\n",
        "        assert cluster_se.shape == (len(X_cluster), p) # <-- SOLUTION\n",
        "        # Calculate the total contribution for the lth cluster\n",
        "        # and add to  the running total\n",
        "        within_distance += np.sum(cluster_se) # <-- SOLUTION\n",
        "\n",
        "    return within_distance\n",
        "\n",
        "# EDIT THIS FUNCTION\n",
        "def compute_centroids(K, X, labels):\n",
        "    \"\"\"\n",
        "    Compute the centroids of the clustered points X.\n",
        "\n",
        "    Parameters:\n",
        "        K (int): The number of clusters.\n",
        "        X (np.ndarray): Samples matrix, shape (N, p).\n",
        "        labels (np.ndarray): Cluster index of each sample, shape (N,).\n",
        "\n",
        "    Returns:\n",
        "        centroids (np.ndarray): Centroids matrix, shape (k, p).\n",
        "    \"\"\"\n",
        "    N, p = X.shape\n",
        "\n",
        "    centroids = np.zeros((K, p))\n",
        "\n",
        "    # EDIT THE NEXT LINES\n",
        "    for label in range(K):\n",
        "        cluster_X_l = X[labels == label] # <-- SOLUTION\n",
        "        # if cluster is empty, set all features values to Inf\n",
        "        if cluster_X_l.shape[0] == 0:\n",
        "          centroids[label] = np.Inf # <-- SOLUTION\n",
        "        else:\n",
        "          centroids[label] = cluster_X_l.mean(axis=0) # <-- SOLUTION\n",
        "\n",
        "\n",
        "    return centroids\n",
        "# EDIT THIS FUNCTION\n",
        "def kmeans_assignments(centroids, X):\n",
        "    \"\"\"\n",
        "    Assign every sample  the index of the closest centroid.\n",
        "\n",
        "    Parameters:\n",
        "        centroids (np.ndarray): Centroids matrix, shape (k, p).\n",
        "        X (np.ndarray): Samples matrix, shape (N, p).\n",
        "\n",
        "    Returns:\n",
        "        labels (np.ndarray): Cluster index of each sample, shape (N,).\n",
        "    \"\"\"\n",
        "    K, p = centroids.shape\n",
        "    N, _ = X.shape\n",
        "\n",
        "    # Compute distances between data points and centroids. Assumed shape: (k, N).\n",
        "    distances = np.vstack([np.linalg.norm(X - c, axis=1) for c in centroids]) # <-- SOLUTION\n",
        "    assert distances.shape == (K, N), f\"Unexpected shape {distances.shape} != {(K, N)}\"\n",
        "\n",
        "    labels = np.argmin(distances, axis=0) # <-- SOLUTION\n",
        "\n",
        "    return labels\n",
        "\n",
        "# EDIT THIS FUNCTION\n",
        "def kmeans_clustering(X, K, rng,\n",
        "                      max_iters=1000,\n",
        "                      epsilon=0.0,\n",
        "                      callback=None,\n",
        "                      print_empty=False):\n",
        "    \"\"\"\n",
        "    Apply k-means clustering algorithm on the samples in `X` to discover\n",
        "    k clusters.\n",
        "\n",
        "    Parameters:\n",
        "        X (np.ndarray): Samples matrix, shape (N, p).\n",
        "        K (int): The number of clusters.\n",
        "        rng (np.random.default_rng): A random number generator.\n",
        "        max_iters (int): Maximum number of iterations.\n",
        "        epsilon (float): The convergence threshold.\n",
        "        callback (Callable): Function to be called on the labels and centroids.\n",
        "        print_empty (bool): Whether to print the number of empty clusters.\n",
        "\n",
        "    Returns:\n",
        "        labels (np.ndarray): Cluster index of each sample, shape (N,).\n",
        "        centroids (np.ndarray): Centroids matrix, shape (k, p).\n",
        "        within_distance (float): Within-cluster distance.\n",
        "    \"\"\"\n",
        "    # Step 0: randomly initialise the cluster assignments.\n",
        "    labels = rng.choice(K, size=len(X), replace=True) # <-- SOLUTION\n",
        "    # initialise within_distance\n",
        "    within_distance = np.inf\n",
        "\n",
        "    for _ in range(max_iters):\n",
        "        # Step 1: compute the centroids\n",
        "        centroids = compute_centroids(K, X, labels) # <-- SOLUTION\n",
        "\n",
        "        if callback != None:\n",
        "            callback(labels, centroids)\n",
        "\n",
        "        # Step 2: reassignments.\n",
        "        new_labels = kmeans_assignments(centroids, X) # <-- SOLUTION\n",
        "\n",
        "        # Step 3: termination check\n",
        "        _within_distance = compute_within_distance(centroids, X, labels)\n",
        "        if all(labels == new_labels) or abs(_within_distance - within_distance) < epsilon: # <-- SOLUTION\n",
        "            break\n",
        "\n",
        "        labels = new_labels\n",
        "        within_distance = _within_distance\n",
        "\n",
        "\n",
        "    num_empty = K - len(np.unique(labels))\n",
        "    if (num_empty != 0) & print_empty :\n",
        "      print(num_empty, 'empty cluster(s) present.')\n",
        "\n",
        "    return labels, centroids, within_distance\n",
        "\n",
        "# EDIT THIS FUNCTION\n",
        "def kmeans_clustering_multi_runs(X, K, max_iters=100,\n",
        "                                 epsilon=0.0,\n",
        "                                 n_runs=100, seed=0):\n",
        "    \"\"\"\n",
        "    Perform multiple runs (with different initialisations) of kmeans algorithm\n",
        "    and return the best clustering using the within-cluster distance.\n",
        "\n",
        "    Parameters:\n",
        "        X (np.ndarray): Samples matrix, shape (N, p).\n",
        "        K (int): The number of clusters.\n",
        "        max_iters (int): Maximum number of iterations.\n",
        "        epsilon (float): The convergence threshold.\n",
        "        n_runs (int): The number of runs of kmeans with different initialisations.\n",
        "        seed (int): A seed value before starting the n_runs loop.\n",
        "\n",
        "    Returns:\n",
        "        labels (np.ndarray): Cluster index of each sample, shape (N,).\n",
        "        centroids (np.ndarray): Centroids matrix, shape (k, p).\n",
        "        intermediates (list): The clustering trajectory for the best clustering.\n",
        "        within_distance (float): Within-cluster distance.\n",
        "    \"\"\"\n",
        "    # We fix the seed once before starting the n_runs.\n",
        "    rng = np.random.default_rng(seed=seed)\n",
        "    min_within_distance = np.inf\n",
        "    best_clustering = (None, None, None)\n",
        "\n",
        "    for _ in range(n_runs):\n",
        "\n",
        "        # define callback function\n",
        "        intermediates = []\n",
        "        callback = lambda labels, centroids: intermediates.append((labels, centroids))\n",
        "\n",
        "        labels, centroids, within_distance = kmeans_clustering(X, K, rng,\n",
        "                                                    max_iters=max_iters,\n",
        "                                                    epsilon=epsilon,\n",
        "                                                    callback=callback) # <-- SOLUTION\n",
        "\n",
        "        if within_distance < min_within_distance:\n",
        "            # store results so they are not overwritten\n",
        "            best_clustering = labels, centroids, intermediates\n",
        "            min_within_distance = within_distance\n",
        "\n",
        "    labels, centroids, intermediates = best_clustering\n",
        "    return labels, centroids, intermediates, min_within_distance"
      ],
      "metadata": {
        "id": "58iM4NOjNpVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xudbycrErgiI"
      },
      "source": [
        "### Using $K$-means for spectral clustering\n",
        "\n",
        "Let us define a $N\\times r$ feature matrix $X$ that contains the first $r>0$ Laplacian eigenvectors (corresponding to non-zero eigenvalues) as columns. For spectral clustering, it is recommended to first obtain a normalised feature matrix $Y$ from $X$ following the famous paper by [Ng, Jordan, and Weiss 2001](https://papers.nips.cc/paper/2001/file/801272ee79cfde7fa5960571fee36b9b-Paper.pdf):\n",
        "\n",
        "$$Y_{ij}=\\frac{X_{ij}}{\\sqrt{\\sum_{j=1}^r X_{ij}^2}},$$\n",
        "\n",
        "such that the rows of $Y$ have unit lengths. We compute $Y$ for $r=10$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXcVr-1N5LRM"
      },
      "outputs": [],
      "source": [
        "## EDIT THIS CELL\n",
        "\n",
        "# stack first r eigenvectors in columns to get spectral embeddings\n",
        "r = 10\n",
        "X =  # <-- EDIT THIS LINE\n",
        "\n",
        "# normalise to unit length\n",
        "Y =  # <-- EDIT THIS LINE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1qQBOb8qBVs"
      },
      "source": [
        "Treating each row of $Y$ as a point in $\\mathbb{R}^r$ gives us $r$-dimensional embeddings for the nodes in the network. We can then cluster the network using $k$-means. Our question is whether we can retrieve the four node types using spectral clustering, so we set $K=4$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1AYlFYFrf0X"
      },
      "outputs": [],
      "source": [
        "## EDIT THIS CELL\n",
        "\n",
        "# apply K-means clustering to spectral embeddings\n",
        "spectral_partition, _, _, _ =  # <-- EDIT THIS LINE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhWdGEK_qzvA"
      },
      "source": [
        "Already through visualising the partition, it seems like there is a low correspondence to the node types when compared to the visualsiations above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LsuDUFZurDcQ"
      },
      "outputs": [],
      "source": [
        "# use different colours for spectral partition\n",
        "cmap_partition = {0 : \"purple\", 1 : \"yellow\", 2 : \"pink\", 3 : \"green\"}\n",
        "color_partition = [cmap_partition[i] for i in spectral_partition]\n",
        "\n",
        "# plot graph\n",
        "ax = plot_graph(adjacency=A, x_coord=x, y_coord=d, node_size=d, node_color=color_partition)\n",
        "\n",
        "# create legend for spectral partition\n",
        "partition_legend = [plt.Line2D([0], [0], marker='o', color='w', label=\"Community 1\", markerfacecolor=\"purple\"),\n",
        "                      plt.Line2D([0], [0], marker='o', color='w', label=\"Community 2\", markerfacecolor=\"yellow\"),\n",
        "                      plt.Line2D([0], [0], marker='o', color='w', label=\"Community 3\", markerfacecolor=\"pink\"),\n",
        "                      plt.Line2D([0], [0], marker='o', color='w', label=\"Community 4\", markerfacecolor=\"green\")]\n",
        "plt.legend(handles=partition_legend, title='Spectral partition')\n",
        "\n",
        "# plot labels\n",
        "ax.set(xlabel=\"Normalised Fiedler eigenvector\", ylabel=\"Degree\", title=r\"Spectral partition with $K=4$\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAXL9BBvryOy"
      },
      "source": [
        "We also observe that the degree distributions of the different clusters in the spectral partition are very similar, so unlike the degree distributions according to the different node types discussed above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7M7lGPInr762"
      },
      "outputs": [],
      "source": [
        "# visualise degree distributions for the four spectral communities\n",
        "fig, ax = plt.subplots(1)\n",
        "bins = np.arange(0,d.max()+2,2)\n",
        "ax.hist(d[spectral_partition==0], bins=bins,alpha=0.5, edgecolor='black', label=r\"Community 1\", color=\"purple\")\n",
        "ax.hist(d[spectral_partition==1], bins=bins,alpha=0.5, edgecolor='black', label=r\"Community 2\", color=\"yellow\")\n",
        "ax.hist(d[spectral_partition==2], bins=bins,alpha=0.5, edgecolor='black', label=r\"Community 2\", color=\"pink\")\n",
        "ax.hist(d[spectral_partition==3], bins=bins,alpha=0.5, edgecolor='black', label=r\"Community 2\", color=\"green\")\n",
        "ax.set(xlabel=\"Degree\",ylabel=\"Frequency\",title=\"Degree distribution for for spectral partition with $K=4$\")\n",
        "ax.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCV4hOKyyfAK"
      },
      "source": [
        "However, it is expected that the spectral partition is not consistent with the partition of node types because it actually reflects the physical positions of the neurons in the organism (we can't show this here). This is the case because the spectral partition is obtained from the Laplacian that encodes the connectivity of neurons in a physical organism."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YS9wfA582vjG"
      },
      "source": [
        "### Using Normalised Variation of Information for comparing partitions\n",
        "\n",
        "We implement the Normalised Variation of Information (NVI) to compare the spectral partition with the node types (which constitutes an alternative partition of the network). For two partitions $X$ and $Y$ you can compute the entropies $\\mathrm H(X)$ and $\\mathrm H(Y)$ and the mutual information $\\mathrm{MI}(X,Y)$. The $\\mathrm{NVI}$ is then given by:\n",
        "\n",
        "$$\\mathrm{NVI}(X,Y)=\\frac{ \\mathrm H(X)+\\mathrm H(Y)-2 \\mathrm{MI}(X,Y)}{\\mathrm H(X)+\\mathrm H(Y)- \\mathrm{MI}(X,Y)}$$\n",
        "\n",
        "The NVI is a metric on the space of partitions and ranges between 0 (partitions are the same) to 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdPehkmX1jlf"
      },
      "outputs": [],
      "source": [
        "## EDIT THIS CELL\n",
        "\n",
        "def compute_NVI(partition_1,partition_2):\n",
        "    \"\"\"Computes NVI of two partitions.\n",
        "\n",
        "    Parameters:\n",
        "        partition_1 (np.array): Encoding for partition 1.\n",
        "        partition_2 (np.array): Encoding for partition 2.\n",
        "\n",
        "    Returns:\n",
        "        NVI (float): NVI of the two partitions.\n",
        "    \"\"\"\n",
        "    # check if partitions are defined on the same underlying space\n",
        "    assert len(partition_1) == len(partition_2), \"Partition arrays must have same length\"\n",
        "\n",
        "    # get number of points\n",
        "    N = len(partition_1)\n",
        "\n",
        "    # get communities as sets from partition 1\n",
        "    communities_1 = []\n",
        "    for index in np.unique(partition_1):\n",
        "        community = set(np.where(partition_1==index)[0])\n",
        "        communities_1.append(community)\n",
        "\n",
        "    # get communities as sets from partition 2\n",
        "    communities_2 = []\n",
        "    for index in np.unique(partition_2):\n",
        "        community = set(np.where(partition_2==index)[0])\n",
        "        communities_2.append(community)\n",
        "\n",
        "    # compute number of communities\n",
        "    n1 = len(communities_1)\n",
        "    n2 = len(communities_2)\n",
        "\n",
        "    # compute probabilities for the two partitions\n",
        "    p1 =  # <-- EDIT THIS LINE\n",
        "    p2 =  # <-- EDIT THIS LINE\n",
        "\n",
        "    # compute joint probabilities\n",
        "    p12 = np.zeros((n1,n2))\n",
        "    for i in range(n1):\n",
        "        for j in range(n2):\n",
        "            p12[i,j] =  # <-- EDIT THIS LINE\n",
        "\n",
        "    # compute entropy\n",
        "    H1 =  # <-- EDIT THIS LINE\n",
        "    H2 =  # <-- EDIT THIS LINE\n",
        "\n",
        "    # compute mutual information\n",
        "    MI = 0\n",
        "    for i in range(n1):\n",
        "        for j in range(n2):\n",
        "            if p12[i,j] > 0:\n",
        "                MI += # <-- EDIT THIS LINE\n",
        "\n",
        "    # compute NVI\n",
        "    NVI = # <-- EDIT THIS LINE\n",
        "\n",
        "    return NVI\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JLEg_dTw3Yb"
      },
      "source": [
        "You can check your implementation with the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4U7jBUw9w2yn"
      },
      "outputs": [],
      "source": [
        "# check for two test cases\n",
        "partition_1 = np.asarray([0,0,0,1,1,1])\n",
        "partition_2 = np.asarray([0,1,0,1,0,1])\n",
        "npt.assert_allclose(compute_NVI(partition_1,partition_2), 0.9574079427713601)\n",
        "\n",
        "partition_3 = np.asarray([3,2,0,1,1,1])\n",
        "partition_4 = np.asarray([0,0,0,0,0,1])\n",
        "npt.assert_allclose(compute_NVI(partition_3,partition_4), 0.9152282678756786)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMydm4TTwdN_"
      },
      "source": [
        "We can now compute the NVI between the spectral partition and the node type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdmBhrPr4aof"
      },
      "outputs": [],
      "source": [
        "## EDIT THIS CELL\n",
        "print(\"NVI of spectral partition and node types:\", ...) # <-- EDIT THIS LINE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFUd2mMZyM-H"
      },
      "source": [
        "The NVI confirms our observations that the spectral partition is actually not very similar to the partition of node types.\n",
        "\n",
        "We conclude here by double-checking some of the metric properties of the NVI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNs9GqPVvZSE"
      },
      "outputs": [],
      "source": [
        "# we can check that the NVI is symmetric\n",
        "npt.assert_allclose(compute_NVI(spectral_partition,node_type), compute_NVI(node_type,spectral_partition))\n",
        "\n",
        "# and that the distance of a partition to itself is zero\n",
        "npt.assert_allclose(compute_NVI(spectral_partition,spectral_partition), 0, atol=1e-15)\n",
        "npt.assert_allclose(compute_NVI(node_type,node_type), 0, atol=1e-15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGmshKQhzBZ8"
      },
      "source": [
        "## Analysing the directed C. Elegans network\n",
        "\n",
        "As the node types I, S, M and U actually correspond to different functionalities of neurons in the organism, we expect that they play different \"roles\" in the network. We saw this already when looking at the degree distribution of the different types, where the inter neurons (I) had the the highest degrees.\n",
        "\n",
        "We can improve our analysis by looking not only at the degree in the undirected network, but studing the in- and out-degrees in the directed C. Elegans network.\n",
        "\n",
        "Let us start with compiling the directed unweighted C. Elegans network whose adjacency matrix we denote by $B$ such that $B_{ij}=1$ if $A_{\\text{raw}, ij}>0$ and $B_{ij}=0$ otherwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Yu9vdYizyYF"
      },
      "outputs": [],
      "source": [
        "## EDIT THIS CELL\n",
        "\n",
        "# obtain directed adjacency matrix\n",
        "B = # <-- EDIT THIS LINE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jirl5GeNz83r"
      },
      "source": [
        "We can then compute the in-degree $d_{\\mathrm{in}}$ and out-degree $d_{\\mathrm{out}}$ of the directed graph corresponding to $B$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69mDZRsz0BDP"
      },
      "outputs": [],
      "source": [
        "## EDIT THIS CELL\n",
        "\n",
        "# compute in-degree\n",
        "d_in = # <-- EDIT THIS LINE\n",
        "\n",
        "# compute out-degree\n",
        "d_out = # <-- EDIT THIS LINE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-htkYrA0XSq"
      },
      "source": [
        "We now plot the neurons in a scatter plot with x-coordinate given by the in-degree and y-coordinate given by the out-degree."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5tmBlLX0mMA"
      },
      "outputs": [],
      "source": [
        "# scatter plot in- and out-degrees\n",
        "fig, ax = plt.subplots(1)\n",
        "\n",
        "# plot nodes\n",
        "scatter = ax.scatter(d_in, d_out, c=color_type, zorder=10)\n",
        "\n",
        "# plot legend\n",
        "plt.legend(handles=types_legend, title='Types')\n",
        "\n",
        "# plot labels\n",
        "ax.set(xlabel=\"In-degree\", ylabel=\"Out-degree\", title=\"Node degrees in directed graph\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWP3zFD2Cmgg"
      },
      "source": [
        "We can see that the combination of in- and out-degreens helps us to distinguish the different types. Most strikingly, the muscles have a very low out-degree (mostly 0) and only incoming nodes as they sit at the bottom of the hierarchy in the nervous system of C. Elegans.\n",
        "\n",
        "Next we will apply k-means clustering to a feature matix $Z$ whose columns correspond to the in- and out-degrees and we apply a log-transformation as the degree distribution is skewed. In particular, the first column is given by $\\log(1+d_\\mathrm{in})$ and second column by $\\log(1+d_\\mathrm{out})$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_psP_xjE3Byq"
      },
      "outputs": [],
      "source": [
        "## EDIT THIS CELL\n",
        "\n",
        "# combine log-transformed in- and out-degree to new feature matrix\n",
        "Z = # <-- EDIT THIS LINE\n",
        "\n",
        "# apply k-means clustering to in and out-degrees\n",
        "in_out_partition, _, _, _ = # <-- EDIT THIS LINE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0x-Q2jBWDJMy"
      },
      "source": [
        "We can visualise the obtained clustering in a scatter plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqyctMpx3O6D"
      },
      "outputs": [],
      "source": [
        "# use different colours for spectral partition\n",
        "cmap_partition = {0 : \"purple\", 1 : \"yellow\", 2 : \"pink\", 3 : \"green\"}\n",
        "color_partition = [cmap_partition[i] for i in in_out_partition]\n",
        "\n",
        "# scatter plot in- and out-degrees\n",
        "fig, ax = plt.subplots(1)\n",
        "\n",
        "# plot nodes\n",
        "scatter = ax.scatter(d_in, d_out, c=color_partition, zorder=10)\n",
        "\n",
        "# create legend for clustering\n",
        "partition_legend = [plt.Line2D([0], [0], marker='o', color='w', label=\"Cluster 1\", markerfacecolor=\"purple\"),\n",
        "                      plt.Line2D([0], [0], marker='o', color='w', label=\"Cluster 2\", markerfacecolor=\"yellow\"),\n",
        "                      plt.Line2D([0], [0], marker='o', color='w', label=\"Cluster 3\", markerfacecolor=\"pink\"),\n",
        "                      plt.Line2D([0], [0], marker='o', color='w', label=\"Cluster 4\", markerfacecolor=\"green\")]\n",
        "plt.legend(handles=partition_legend, title='Degree-based partition')\n",
        "\n",
        "# plot labels\n",
        "ax.set(xlabel=\"In-degree\", ylabel=\"Out-degree\", title=\"Degree-based partition\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing our results to the previous plot we observe that the clusters partially match the different node types. For example, Cluster 1 captures the muscles (U) and Cluster 4 captures many of the high in-degree inter neurons (I). However, the partition does not fully distinguish the different node types."
      ],
      "metadata": {
        "id": "1klKQ5QZoOFl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cm2NO8sZDRKT"
      },
      "source": [
        "To quantify the correspondence to the ground truth node types, we evaluate the NVI of the degree-based partition and the node types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vlFMmv43WGL"
      },
      "outputs": [],
      "source": [
        "## EDIT THIS CELL\n",
        "print(\"NVI of in-degree out-degree partition and node types:\", ...) # <-- EDIT THIS LINE"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We observe a slightly lower NVI value as compared to spectral clustering of the undirected graph. This suggests that the directed graph contains important information about the functionality of the different nodes.\n",
        "\n",
        "The reason for this is that the analysis of in-degree and out-degree patterns of nodes can recover the different roles that nodes play in the network. If you want to read more on how to distinguish nodes in a directed graph based on in-coming and out-going paths you can have a look at a paper by Kathryn Cooper and Mauricio Barahona on *Role-based similarity in directed networks*: https://arxiv.org/abs/1012.2726"
      ],
      "metadata": {
        "id": "BbZKeIrcr4T9"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}